{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"10k_diabetes/diab_train.csv\",\n",
    "                       na_values = [\"?\", \"Not Available\", \"Not Mapped\"])\n",
    "df_test = pd.read_csv(\"10k_diabetes/diab_test.csv\")\n",
    "df_validate = pd.read_csv(\"10k_diabetes/diab_validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 52)\n",
      "Unnamed: 0                   int64\n",
      "race                        object\n",
      "gender                      object\n",
      "age                         object\n",
      "weight                      object\n",
      "admission_type_id           object\n",
      "discharge_disposition_id    object\n",
      "admission_source_id         object\n",
      "time_in_hospital             int64\n",
      "payer_code                  object\n",
      "medical_specialty           object\n",
      "num_lab_procedures           int64\n",
      "num_procedures               int64\n",
      "num_medications              int64\n",
      "number_outpatient            int64\n",
      "number_emergency             int64\n",
      "number_inpatient             int64\n",
      "diag_1                      object\n",
      "diag_2                      object\n",
      "diag_3                      object\n",
      "number_diagnoses             int64\n",
      "max_glu_serum               object\n",
      "A1Cresult                   object\n",
      "metformin                   object\n",
      "repaglinide                 object\n",
      "nateglinide                 object\n",
      "chlorpropamide              object\n",
      "glimepiride                 object\n",
      "acetohexamide               object\n",
      "glipizide                   object\n",
      "glyburide                   object\n",
      "tolbutamide                 object\n",
      "pioglitazone                object\n",
      "rosiglitazone               object\n",
      "acarbose                    object\n",
      "miglitol                    object\n",
      "troglitazone                object\n",
      "tolazamide                  object\n",
      "examide                     object\n",
      "citoglipton                 object\n",
      "insulin                     object\n",
      "glyburide.metformin         object\n",
      "glipizide.metformin         object\n",
      "glimepiride.pioglitazone    object\n",
      "metformin.rosiglitazone     object\n",
      "metformin.pioglitazone      object\n",
      "change                      object\n",
      "diabetesMed                 object\n",
      "readmitted                   int64\n",
      "diag_1_desc                 object\n",
      "diag_2_desc                 object\n",
      "diag_3_desc                 object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_txt = [\"diag_1_desc\",\n",
    "           \"diag_2_desc\",\n",
    "           \"diag_3_desc\"]\n",
    "\n",
    "type_drop = [\"discharge_disposition_id\",\n",
    "           \"medical_specialty\",]\n",
    "\n",
    "type_cat = [\"race\",\n",
    "          \"gender\",\n",
    "          \"age\",\n",
    "          \"weight\",\n",
    "          \"admission_type_id\",\n",
    "        \"admission_source_id\",\n",
    "        \"payer_code\",\n",
    "          \"max_glu_serum\",\n",
    "           \"A1Cresult\",\n",
    "           \"metformin\",\n",
    "           \"repaglinide\",\n",
    "           \"nateglinide\",\n",
    "           \"chlorpropamide\",\n",
    "           \"glimepiride\",\n",
    "           \"acetohexamide\",\n",
    "           \"glipizide\",\n",
    "           \"glyburide\",\n",
    "           \"tolbutamide\",\n",
    "           \"pioglitazone\",\n",
    "           \"rosiglitazone\",\n",
    "           \"acarbose\",\n",
    "           \"miglitol\",\n",
    "           \"troglitazone\",\n",
    "           \"tolazamide\",\n",
    "           \"examide\",\n",
    "           \"citoglipton\",\n",
    "           \"insulin\",\n",
    "           \"glyburide.metformin\",\n",
    "           \"glipizide.metformin\",\n",
    "           \"glimepiride.pioglitazone\",\n",
    "           \"metformin.rosiglitazone\",\n",
    "           \"metformin.pioglitazone\",\n",
    "           \"change\",\n",
    "           \"diabetesMed\"]\n",
    "\n",
    "type_le = [\"age\", \"weight\", \"A1Cresult\"]\n",
    "\n",
    "type_int = [\"time_in_hospital\",\n",
    "           \"num_lab_procedures\",\n",
    "           \"num_procedures\",\n",
    "           \"num_medications\",\n",
    "           \"number_outpatient\",\n",
    "           \"number_emergency\",\n",
    "           \"number_inpatient\",\n",
    "           \"number_diagnoses\"]\n",
    "\n",
    "type_float = [\"diag_1\",\n",
    "             \"diag_2\",\n",
    "             \"diag_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_df(df):\n",
    "    y = df[\"readmitted\"]\n",
    "    df = df.drop(columns=['readmitted', 'Unnamed: 0'])\n",
    "    df = df.drop(columns=type_drop)\n",
    "    \n",
    "    #Convert data types\n",
    "    for i in type_int:\n",
    "        #df_train[i] = df_train[i].astype('int32')\n",
    "        df[i] = pd.to_numeric(df[i], errors='coerce', downcast='integer')\n",
    "\n",
    "    for i in type_float:\n",
    "        df[i] = pd.to_numeric(df[i], errors='coerce', downcast='float')\n",
    "        \n",
    "    for i in type_txt:\n",
    "        df[i] = df[i].astype('str')\n",
    "        df[i] = df[i].str.lower()\n",
    "        \n",
    "    for i in type_cat:\n",
    "        df[i] = df[i].astype('str')\n",
    "        df[i] = df[i].str.lower()\n",
    "        \n",
    "    #Combine descriptions\n",
    "    tmp = df[type_txt[0]] + \" \" + df[type_txt[1]] + \" \" + df[type_txt[2]]\n",
    "    tmp = pd.DataFrame({'description':tmp})\n",
    "    df = pd.concat([tmp, df], axis = 1)\n",
    "    df = df.drop(columns = type_txt)\n",
    "    \n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def get_transforms(df, impute=True, imp=\"mean\"):\n",
    "    ll = type_txt.copy()\n",
    "    ll.append('description')\n",
    "    \n",
    "    #Get features that are categorical and create oh encoding\n",
    "    ohe_mask = df.dtypes==object\n",
    "    txt_mask = [i not in ll for i in df.columns]\n",
    "    mask = [i == True and j == True for i,j in zip(txt_mask, ohe_mask.tolist())]\n",
    "    col_mask = df.columns[mask]\n",
    "    \n",
    "    #Generate OneHotEncoder\n",
    "    ohe = [OneHotEncoder().fit(df[i].values.reshape(-1,1)) for i in col_mask]\n",
    "    enc = [ohe[i].transform(df[name].values.reshape(-1,1)).toarray() for i,name in enumerate(col_mask)]\n",
    "    \n",
    "    #Concat transformed features\n",
    "    tmp = np.concatenate(enc, axis = 1)\n",
    "    tmp = pd.DataFrame(tmp, columns = ['ohe' + str(i) for i in range(tmp.shape[1])])\n",
    "    \n",
    "    #Append to dataframe and drop categorical features that have been transformed\n",
    "    df = pd.concat([df.reset_index(drop=True), tmp.reset_index(drop=True)], axis = 1)\n",
    "    df = df.drop(columns=col_mask)\n",
    "    \n",
    "    #Impute missing values\n",
    "    if impute:\n",
    "        idx = pd.isnull(df).any().tolist()\n",
    "        print(\"Impute values for the following attributes\")\n",
    "        print(df.columns[idx])\n",
    "\n",
    "        df_imp = SimpleImputer(strategy=imp).fit_transform(df.loc[:,idx])\n",
    "        df.loc[:,idx] = df_imp\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def tokenize(x,stem = False):\n",
    "    rm_word = stopwords.words('english')\n",
    "    rm_word.extend(',')\n",
    "    \n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(i) for i in list(filter(lambda x: x not in rm_word, word_tokenize(x)))]\n",
    "    else:\n",
    "        return list(filter(lambda x: x not in rm_word, word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "def get_nlp(df, n_gram_range=(1,1), stem=False, tfidf=False):\n",
    "    #NLP \n",
    "    tfidfTransformer = TfidfTransformer()\n",
    "    count_vect = CountVectorizer(ngram_range=n_gram_range)\n",
    "    \n",
    "    if not n_gram_range == None:\n",
    "        #Filter description and possibly stem\n",
    "        df['description'] = df['description'].apply(lambda txt: ' '.join(tokenize(txt, stem=stem)))\n",
    "        \n",
    "        X_transformed = count_vect.fit_transform(df['description'].values)\n",
    "        \n",
    "        if tfidf:\n",
    "            X_transformed = tfidfTransformer.fit_transform(X_transformed)\n",
    "            \n",
    "        tmp = pd.DataFrame(X_transformed.A, columns = ['nlp' + str(i) for i in range(X_transformed.shape[1])])    \n",
    "        df = pd.concat([df.reset_index(drop=True), tmp.reset_index(drop=True)], axis = 1)\n",
    "        df = df.drop(columns=['description'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(DF,df,y):\n",
    "    idx = np.cumsum([i.shape[0] for i in DF])\n",
    "    X_train = df.iloc[0:idx[0],]\n",
    "    y_train = y.iloc[0:idx[0],]\n",
    "    \n",
    "    X_val = df.iloc[idx[0]:idx[1],]\n",
    "    y_val = y.iloc[idx[0]:idx[1],]\n",
    "    \n",
    "    X_test = df.iloc[idx[1]:idx[2],]\n",
    "    y_test = y.iloc[idx[1]:idx[2],]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "\n",
    "def prep_data(df, col_mask, ohe, count_vect=None, tfidf_trans=None, impute=True, imp = \"mean\", nlp=False, stem=False, tfidf = False, n_gram_range = None):\n",
    "        \n",
    "    enc = [ohe[i].transform(df[name].values.reshape(-1,1)).toarray() for i,name in enumerate(col_mask)]\n",
    "    \n",
    "    #Concat transformed features\n",
    "    tmp = np.concatenate(enc, axis = 1)\n",
    "    tmp = pd.DataFrame(tmp, columns = ['ohe' + str(i) for i in range(tmp.shape[1])])\n",
    "    \n",
    "    #Append to dataframe and drop categorical features that have been transformed\n",
    "    df = pd.concat([df, tmp], axis = 1)\n",
    "    df = df.drop(columns=col_mask)\n",
    "    \n",
    "    #NLP processing\n",
    "    tfidfTransformer = TfidfTransformer()\n",
    "    count_vect = CountVectorizer(ngram_range=n_gram_range)\n",
    "    if not nlp:\n",
    "        print(\"Mode: No NLP\")\n",
    "        df = df.drop(columns = type_txt)\n",
    "    else:\n",
    "        print(\"Mode: NLP, Stem: {}\".format(stem))\n",
    "                \n",
    "        #Filter description and possibly stem\n",
    "        df['description'] = df['description'].apply(lambda txt: ' '.join(tokenize(txt, stem=stem)))\n",
    "        \n",
    "        if train:\n",
    "            X_transformed = count_vect.fit_transform(df['description'].values)\n",
    "        else:\n",
    "            X_transformed = count_trans.transform(df['description'].values)\n",
    "            #count_trans._validate_vocabulary()      \n",
    "\n",
    "        #Wheter to use tfidf instead of counts\n",
    "        if tfidf:\n",
    "            if train:\n",
    "                X_transformed = tfidfTransformer.fit_transform(X_transformed)\n",
    "            else:\n",
    "                X_transformed = tfidf_trans.transform(X_transformed)\n",
    "  \n",
    "        tmp = pd.DataFrame(X_transformed.A, columns = ['nlp' + str(i) for i in range(X_transformed.shape[1])])    \n",
    "        #tmp = pd.DataFrame(X_transformed.A, columns = ['nlp' + i for i in count_vect.get_feature_names()])    \n",
    "        df = pd.concat([df, tmp], axis = 1)\n",
    "        df = df.drop(columns=['description'])\n",
    "    \n",
    "    if train:\n",
    "        return df, y, ohe, count_vect, tfidfTransformer\n",
    "    else:\n",
    "        return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impute values for the following attributes\n",
      "Index(['diag_1', 'diag_2', 'diag_3'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "DF = [df_train, df_validate, df_test]\n",
    "df = pd.concat(DF, axis = 0)\n",
    "nlp = True\n",
    "\n",
    "df,y = prep_df(df)\n",
    "df = get_transforms(df, impute=True, imp=\"mean\")\n",
    "if nlp:\n",
    "    df = get_nlp(df, n_gram_range=(1,1), stem=False, tfidf=False)\n",
    "else:\n",
    "    df = df.drop(columns=['description'])\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split_df(DF,df,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.639 0.6385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=20, min_samples_split=5)\n",
    "clf.fit(X_train.values, y_train)\n",
    "\n",
    "pred_val = clf.predict(X_val.values)\n",
    "pred_test = clf.predict(X_test.values)\n",
    "\n",
    "acc_val = accuracy_score(y_val, pred_val)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(acc_val, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_dict = {'max_depth': [None, 10, 50],\n",
    "             'min_samples_split': [2, 10],\n",
    "             'nlp': [True, False],\n",
    "             'stem': [True, False],\n",
    "             'n_gram_range': [None, (1,1), (2,2)],}\n",
    "\n",
    "param_grid = list(ParameterGrid(param_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in param_grid:\n",
    "\n",
    "    if not param['nlp'] and param['stem']:\n",
    "        #Skip this parameter setting since unreasonable\n",
    "        continue\n",
    "        \n",
    "    if param['nlp'] and param['n_gram_range'] == None:\n",
    "        #Skip this parameter setting since unreasonable\n",
    "        continue\n",
    "    \n",
    "    if not param['nlp'] and not param['n_gram_range'] == None:\n",
    "        #Skip this parameter setting since unreasonable\n",
    "        continue\n",
    "    \n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "#Run grid search\n",
    "res = list()\n",
    "n_fold = 2\n",
    "for param in param_grid:\n",
    "\n",
    "    if not param['nlp'] and param['stem']:\n",
    "        #Skip this parameter setting since unreasonable\n",
    "        continue\n",
    "        \n",
    "    if param['nlp'] and param['n_gram_range'] == None:\n",
    "        #Skip this parameter setting since unreasonable\n",
    "        continue\n",
    "    \n",
    "    if not param['nlp'] and not param['n_gram_range'] == None:\n",
    "        #Skip this parameter setting since unreasonable\n",
    "        continue\n",
    "        \n",
    "    X_train, y_train, ohe, count_vect, tfidfTransformer = prep_data(df_train, train=True,\n",
    "                                                                    nlp=param['nlp'], \n",
    "                                                                    stem=param['stem'],\n",
    "                                                                    n_gram_range=param['n_gram_range']) \n",
    "                                                                    \n",
    "    X_val, y_val = prep_data(df_validate, train=False, nlp=True, n_gram_range=(1,1),\n",
    "                             ohe_trans=ohe, count_trans=count_vect, tfidf_trans=tfidfTransformer,\n",
    "                             nlp=param['nlp'], stem=param['stem'], n_gram_range=param['n_gram_range'])                                                       n_gram_range=param['n_gram_range'])\n",
    "    \n",
    "    #CV over grid search\n",
    "    acc_cv = []\n",
    "    for train, val in zip(KFold(n_splits=n_fold).split(X_train), KFold(n_splits=n_fold).split(X_val)):\n",
    "        idx_train,_ = train\n",
    "        idx_val,_ = val\n",
    "        \n",
    "        clf = RandomForestClassifier(max_depth=param['max_depth'], min_samples_split=param['min_samples_split'])\n",
    "        clf.fit(X_train.values[idx_train],y_train[idx_train])\n",
    "        pred_test = clf.predict(X_val.values[idx_val])\n",
    "        acc_cv.append(accuracy_score(y_val[idx_val], pred_test))\n",
    "    \n",
    "    acc = np.mean(acc_cv)\n",
    "    acc_sd = np.std(acc_cv)\n",
    "    print('Mean: {}, Std: {}'.format(acc, acc_sd))\n",
    "    res.append((acc, acc_sd, param))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, count_trans, tfidf_trans = prep_data(df_train, train=True)\n",
    "\n",
    "X_val, y_val = prep_data(df_validate, train = False,\n",
    "                             count_trans=count_trans,\n",
    "                             tfidf_trans=tfidf_trans)\n",
    "\n",
    "X_test, y_test = prep_data(df_validate, train = False,\n",
    "                             count_trans=count_trans,\n",
    "                             tfidf_trans=tfidf_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train.values, y_train)\n",
    "\n",
    "pred_val = clf.predict(X_val.values)\n",
    "pred_test = clf.predict(X_test.values)\n",
    "\n",
    "acc_val = accuracy_score(y_val, pred_val)\n",
    "acc_test = accuracy_score(y_test, pred_test)\n",
    "print(acc_val, acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in res:\n",
    "    print('Mean: {}, Std: {}'.format(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = prep_data(df_train, nlp=params['nlp'], stem=param['stem'], n_gram_range=param['n_gram_range'])\n",
    "conf_mat = confusion_matrix(y_test, pred_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
