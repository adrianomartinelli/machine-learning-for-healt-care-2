{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              text\n0              the cat with the hat sat in the mat\n1                    you have brains in your head \n2                      you have feet in your shoes\n3  You can steer yourself any direction you choose\n4               Think and wonder, wonder and think\n5                           The more that you read\n6                    the more things you will know\n7                          The more that you learn\n8                        the more places you’ll go\n9                     the mat has a cat with a hat",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>the cat with the hat sat in the mat</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>you have brains in your head</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>you have feet in your shoes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You can steer yourself any direction you choose</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Think and wonder, wonder and think</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The more that you read</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>the more things you will know</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The more that you learn</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>the more places you’ll go</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>the mat has a cat with a hat</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 34
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "text = ['the cat with the hat sat in the mat',\n",
    "        'you have brains in your head ',' you have feet in your shoes',\n",
    "        'You can steer yourself any direction you choose' ,\n",
    "        'Think and wonder, wonder and think', 'The more that you read',' the more things you will know',\n",
    "        'The more that you learn' ,'the more places you’ll go',\n",
    "        'the mat has a cat with a hat']\n",
    "\n",
    "txt = pd.DataFrame(text,columns=['text'])\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Flurin\n[nltk_data]     Hidber\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "['i',\n 'me',\n 'my',\n 'myself',\n 'we',\n 'our',\n 'ours',\n 'ourselves',\n 'you',\n \"you're\",\n \"you've\",\n \"you'll\",\n \"you'd\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves',\n 'he',\n 'him',\n 'his',\n 'himself',\n 'she',\n \"she's\",\n 'her',\n 'hers',\n 'herself',\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'they',\n 'them',\n 'their',\n 'theirs',\n 'themselves',\n 'what',\n 'which',\n 'who',\n 'whom',\n 'this',\n 'that',\n \"that'll\",\n 'these',\n 'those',\n 'am',\n 'is',\n 'are',\n 'was',\n 'were',\n 'be',\n 'been',\n 'being',\n 'have',\n 'has',\n 'had',\n 'having',\n 'do',\n 'does',\n 'did',\n 'doing',\n 'a',\n 'an',\n 'the',\n 'and',\n 'but',\n 'if',\n 'or',\n 'because',\n 'as',\n 'until',\n 'while',\n 'of',\n 'at',\n 'by',\n 'for',\n 'with',\n 'about',\n 'against',\n 'between',\n 'into',\n 'through',\n 'during',\n 'before',\n 'after',\n 'above',\n 'below',\n 'to',\n 'from',\n 'up',\n 'down',\n 'in',\n 'out',\n 'on',\n 'off',\n 'over',\n 'under',\n 'again',\n 'further',\n 'then',\n 'once',\n 'here',\n 'there',\n 'when',\n 'where',\n 'why',\n 'how',\n 'all',\n 'any',\n 'both',\n 'each',\n 'few',\n 'more',\n 'most',\n 'other',\n 'some',\n 'such',\n 'no',\n 'nor',\n 'not',\n 'only',\n 'own',\n 'same',\n 'so',\n 'than',\n 'too',\n 'very',\n 's',\n 't',\n 'can',\n 'will',\n 'just',\n 'don',\n \"don't\",\n 'should',\n \"should've\",\n 'now',\n 'd',\n 'll',\n 'm',\n 'o',\n 're',\n 've',\n 'y',\n 'ain',\n 'aren',\n \"aren't\",\n 'couldn',\n \"couldn't\",\n 'didn',\n \"didn't\",\n 'doesn',\n \"doesn't\",\n 'hadn',\n \"hadn't\",\n 'hasn',\n \"hasn't\",\n 'haven',\n \"haven't\",\n 'isn',\n \"isn't\",\n 'ma',\n 'mightn',\n \"mightn't\",\n 'mustn',\n \"mustn't\",\n 'needn',\n \"needn't\",\n 'shan',\n \"shan't\",\n 'shouldn',\n \"shouldn't\",\n 'wasn',\n \"wasn't\",\n 'weren',\n \"weren't\",\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\"]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 35
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'you have brains in your head '"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 36
    }
   ],
   "source": [
    "txt.text.values[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['cat', 'hat', 'sat', 'mat']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 37
    }
   ],
   "source": [
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "filtered = []\n",
    "for w in word_tokenize(txt.text.values[0]):\n",
    "    if w in stopwords.words('english'):\n",
    "        continue\n",
    "    filtered.append(w)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['you', 'can', 'steer', 'yourself', 'ani', 'direct', 'you', 'choos']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "print([stemmer.stem(w) for w in word_tokenize(txt.text.values[3])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'You can steer yourself any direction you choose'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 39
    }
   ],
   "source": [
    "txt.text.values[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts/Occurences\n",
    "\n",
    "The model is simple in that it throws away all of the order information in the words and focuses on the occurrence of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array(['the cat with the hat sat in the mat',\n       'you have brains in your head ', ' you have feet in your shoes',\n       'You can steer yourself any direction you choose',\n       'Think and wonder, wonder and think', 'The more that you read',\n       ' the more things you will know', 'The more that you learn',\n       'the more places you’ll go', 'the mat has a cat with a hat'],\n      dtype=object)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 40
    }
   ],
   "source": [
    "txt.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n        0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n       [0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n       [2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,\n        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n        0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n        0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0,\n        0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n        0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]], dtype=int64)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 41
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_transformed = count_vect.fit_transform(txt.text.values)\n",
    "X_transformed.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 34)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 42
    }
   ],
   "source": [
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "    0  1  2  3  4  5  6  7  8  9       term\n0   0  0  0  0  2  0  0  0  0  0        and\n1   0  0  0  1  0  0  0  0  0  0        any\n2   0  1  0  0  0  0  0  0  0  0     brains\n3   0  0  0  1  0  0  0  0  0  0        can\n4   1  0  0  0  0  0  0  0  0  1        cat\n5   0  0  0  1  0  0  0  0  0  0     choose\n6   0  0  0  1  0  0  0  0  0  0  direction\n7   0  0  1  0  0  0  0  0  0  0       feet\n8   0  0  0  0  0  0  0  0  1  0         go\n9   0  0  0  0  0  0  0  0  0  1        has\n10  1  0  0  0  0  0  0  0  0  1        hat\n11  0  1  1  0  0  0  0  0  0  0       have\n12  0  1  0  0  0  0  0  0  0  0       head\n13  1  1  1  0  0  0  0  0  0  0         in\n14  0  0  0  0  0  0  1  0  0  0       know\n15  0  0  0  0  0  0  0  1  0  0      learn\n16  0  0  0  0  0  0  0  0  1  0         ll\n17  1  0  0  0  0  0  0  0  0  1        mat\n18  0  0  0  0  0  1  1  1  1  0       more\n19  0  0  0  0  0  0  0  0  1  0     places\n20  0  0  0  0  0  1  0  0  0  0       read\n21  1  0  0  0  0  0  0  0  0  0        sat\n22  0  0  1  0  0  0  0  0  0  0      shoes\n23  0  0  0  1  0  0  0  0  0  0      steer\n24  0  0  0  0  0  1  0  1  0  0       that\n25  3  0  0  0  0  1  1  1  1  1        the\n26  0  0  0  0  0  0  1  0  0  0     things\n27  0  0  0  0  2  0  0  0  0  0      think\n28  0  0  0  0  0  0  1  0  0  0       will\n29  1  0  0  0  0  0  0  0  0  1       with\n30  0  0  0  0  2  0  0  0  0  0     wonder\n31  0  1  1  2  0  1  1  1  1  0        you\n32  0  1  1  0  0  0  0  0  0  0       your\n33  0  0  0  1  0  0  0  0  0  0   yourself",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>term</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>and</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>any</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>brains</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>can</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>cat</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>choose</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>direction</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>feet</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>go</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>has</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>hat</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>have</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>head</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>in</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>know</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>learn</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>ll</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>mat</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>more</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>places</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>read</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>sat</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>shoes</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>steer</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>that</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>the</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>things</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>think</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>will</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>with</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>wonder</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>you</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>your</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>yourself</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 43
    }
   ],
   "source": [
    "vectdf =pd.DataFrame(X_transformed.A.T)\n",
    "vectdf['term']=count_vect.get_feature_names()\n",
    "vectdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies\n",
    "\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf(corpus, stop_words=None):\n",
    "    tfidf_vect = TfidfVectorizer() if stop_words is None else TfidfVectorizer(stop_words=stop_words)\n",
    "    X_tf_transformed = tfidf_vect.fit_transform(corpus)\n",
    "    tfidf_vectdf = pd.DataFrame(X_tf_transformed.A.T)\n",
    "    tfidf_vectdf['term'] = tfidf_vect.get_feature_names()\n",
    "    return tfidf_vectdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2         3        4         5         6  \\\n0   0.000000  0.000000  0.000000  0.000000  0.57735  0.000000  0.000000   \n1   0.000000  0.000000  0.000000  0.379309  0.00000  0.000000  0.000000   \n2   0.000000  0.485869  0.000000  0.000000  0.00000  0.000000  0.000000   \n3   0.000000  0.000000  0.000000  0.379309  0.00000  0.000000  0.000000   \n4   0.320449  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n5   0.000000  0.000000  0.000000  0.379309  0.00000  0.000000  0.000000   \n6   0.000000  0.000000  0.000000  0.379309  0.00000  0.000000  0.000000   \n7   0.000000  0.000000  0.485869  0.000000  0.00000  0.000000  0.000000   \n8   0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n9   0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n10  0.320449  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n11  0.000000  0.413033  0.413033  0.000000  0.00000  0.000000  0.000000   \n12  0.000000  0.485869  0.000000  0.000000  0.00000  0.000000  0.000000   \n13  0.280355  0.361355  0.361355  0.000000  0.00000  0.000000  0.000000   \n14  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.502327   \n15  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n16  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n17  0.320449  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n18  0.000000  0.000000  0.000000  0.000000  0.00000  0.403482  0.332153   \n19  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n20  0.000000  0.000000  0.000000  0.000000  0.00000  0.610201  0.000000   \n21  0.376958  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n22  0.000000  0.000000  0.485869  0.000000  0.00000  0.000000  0.000000   \n23  0.000000  0.000000  0.000000  0.379309  0.00000  0.000000  0.000000   \n24  0.000000  0.000000  0.000000  0.000000  0.00000  0.518727  0.000000   \n25  0.607086  0.000000  0.000000  0.000000  0.00000  0.327573  0.269663   \n26  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.502327   \n27  0.000000  0.000000  0.000000  0.000000  0.57735  0.000000  0.000000   \n28  0.000000  0.000000  0.000000  0.000000  0.00000  0.000000  0.502327   \n29  0.320449  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000   \n30  0.000000  0.000000  0.000000  0.000000  0.57735  0.000000  0.000000   \n31  0.000000  0.236841  0.236841  0.369795  0.00000  0.297448  0.244864   \n32  0.000000  0.413033  0.413033  0.000000  0.00000  0.000000  0.000000   \n33  0.000000  0.000000  0.000000  0.379309  0.00000  0.000000  0.000000   \n\n           7         8         9       term  \n0   0.000000  0.000000  0.000000        and  \n1   0.000000  0.000000  0.000000        any  \n2   0.000000  0.000000  0.000000     brains  \n3   0.000000  0.000000  0.000000        can  \n4   0.000000  0.000000  0.415853        cat  \n5   0.000000  0.000000  0.000000     choose  \n6   0.000000  0.000000  0.000000  direction  \n7   0.000000  0.000000  0.000000       feet  \n8   0.000000  0.502327  0.000000         go  \n9   0.000000  0.000000  0.489186        has  \n10  0.000000  0.000000  0.415853        hat  \n11  0.000000  0.000000  0.000000       have  \n12  0.000000  0.000000  0.000000       head  \n13  0.000000  0.000000  0.000000         in  \n14  0.000000  0.000000  0.000000       know  \n15  0.610201  0.000000  0.000000      learn  \n16  0.000000  0.502327  0.000000         ll  \n17  0.000000  0.000000  0.415853        mat  \n18  0.403482  0.332153  0.000000       more  \n19  0.000000  0.502327  0.000000     places  \n20  0.000000  0.000000  0.000000       read  \n21  0.000000  0.000000  0.000000        sat  \n22  0.000000  0.000000  0.000000      shoes  \n23  0.000000  0.000000  0.000000      steer  \n24  0.518727  0.000000  0.000000       that  \n25  0.327573  0.269663  0.262609        the  \n26  0.000000  0.000000  0.000000     things  \n27  0.000000  0.000000  0.000000      think  \n28  0.000000  0.000000  0.000000       will  \n29  0.000000  0.000000  0.415853       with  \n30  0.000000  0.000000  0.000000     wonder  \n31  0.297448  0.244864  0.000000        you  \n32  0.000000  0.000000  0.000000       your  \n33  0.000000  0.000000  0.000000   yourself  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>term</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>and</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.379309</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>any</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.485869</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>brains</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.379309</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>can</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.320449</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.415853</td>\n      <td>cat</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.379309</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>choose</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.379309</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>direction</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.485869</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>feet</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.502327</td>\n      <td>0.000000</td>\n      <td>go</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.489186</td>\n      <td>has</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.320449</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.415853</td>\n      <td>hat</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000000</td>\n      <td>0.413033</td>\n      <td>0.413033</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>have</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000000</td>\n      <td>0.485869</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>head</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.280355</td>\n      <td>0.361355</td>\n      <td>0.361355</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>in</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.502327</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>know</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.610201</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>learn</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.502327</td>\n      <td>0.000000</td>\n      <td>ll</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.320449</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.415853</td>\n      <td>mat</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.403482</td>\n      <td>0.332153</td>\n      <td>0.403482</td>\n      <td>0.332153</td>\n      <td>0.000000</td>\n      <td>more</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.502327</td>\n      <td>0.000000</td>\n      <td>places</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.610201</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>read</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.376958</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>sat</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.485869</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>shoes</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.379309</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>steer</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.518727</td>\n      <td>0.000000</td>\n      <td>0.518727</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>that</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.607086</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.327573</td>\n      <td>0.269663</td>\n      <td>0.327573</td>\n      <td>0.269663</td>\n      <td>0.262609</td>\n      <td>the</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.502327</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>things</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>think</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.502327</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>will</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.320449</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.415853</td>\n      <td>with</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>wonder</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.000000</td>\n      <td>0.236841</td>\n      <td>0.236841</td>\n      <td>0.369795</td>\n      <td>0.00000</td>\n      <td>0.297448</td>\n      <td>0.244864</td>\n      <td>0.297448</td>\n      <td>0.244864</td>\n      <td>0.000000</td>\n      <td>you</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.000000</td>\n      <td>0.413033</td>\n      <td>0.413033</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>your</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.379309</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>yourself</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 45
    }
   ],
   "source": [
    "tfidf(txt.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "           0         1         2        3         4    5         6    7  \\\n0   0.000000  0.707107  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n1   0.477612  0.000000  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n2   0.000000  0.000000  0.000000  0.57735  0.000000  0.0  0.000000  0.0   \n3   0.000000  0.000000  0.000000  0.57735  0.000000  0.0  0.000000  0.0   \n4   0.000000  0.000000  0.707107  0.00000  0.000000  0.0  0.000000  0.0   \n5   0.000000  0.000000  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n6   0.477612  0.000000  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n7   0.000000  0.707107  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n8   0.000000  0.000000  0.000000  0.00000  0.000000  0.0  0.707107  0.0   \n9   0.000000  0.000000  0.000000  0.00000  0.000000  0.0  0.000000  1.0   \n10  0.477612  0.000000  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n11  0.000000  0.000000  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n12  0.000000  0.000000  0.000000  0.00000  0.000000  1.0  0.000000  0.0   \n13  0.561836  0.000000  0.000000  0.00000  0.000000  0.0  0.000000  0.0   \n14  0.000000  0.000000  0.707107  0.00000  0.000000  0.0  0.000000  0.0   \n15  0.000000  0.000000  0.000000  0.57735  0.000000  0.0  0.000000  0.0   \n16  0.000000  0.000000  0.000000  0.00000  0.000000  0.0  0.707107  0.0   \n17  0.000000  0.000000  0.000000  0.00000  0.707107  0.0  0.000000  0.0   \n18  0.000000  0.000000  0.000000  0.00000  0.707107  0.0  0.000000  0.0   \n\n           8        9       term  \n0   0.000000  0.00000     brains  \n1   0.000000  0.57735        cat  \n2   0.000000  0.00000     choose  \n3   0.000000  0.00000  direction  \n4   0.000000  0.00000       feet  \n5   0.707107  0.00000         go  \n6   0.000000  0.57735        hat  \n7   0.000000  0.00000       head  \n8   0.000000  0.00000       know  \n9   0.000000  0.00000      learn  \n10  0.000000  0.57735        mat  \n11  0.707107  0.00000     places  \n12  0.000000  0.00000       read  \n13  0.000000  0.00000        sat  \n14  0.000000  0.00000      shoes  \n15  0.000000  0.00000      steer  \n16  0.000000  0.00000     things  \n17  0.000000  0.00000      think  \n18  0.000000  0.00000     wonder  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>term</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>brains</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.477612</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>cat</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>choose</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>direction</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>feet</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.707107</td>\n      <td>0.00000</td>\n      <td>go</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.477612</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>hat</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>head</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.707107</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>know</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>learn</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.477612</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>mat</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.707107</td>\n      <td>0.00000</td>\n      <td>places</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>read</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.561836</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>sat</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.707107</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>shoes</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.57735</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>steer</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.707107</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>things</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.707107</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>think</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.707107</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>wonder</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 46
    }
   ],
   "source": [
    "tfidf(txt.text.values, stop_words=stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams as Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from nltk import word_tokenize\n",
    "\n",
    "exmpl = word_tokenize(\"The quick brown fox jumps over the lazy dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 48
    }
   ],
   "source": [
    "exmpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "tmp = list(bigrams(exmpl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'The-quick'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 50
    }
   ],
   "source": [
    "\"-\".join(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('The', 'quick', 'brown'),\n ('quick', 'brown', 'fox'),\n ('brown', 'fox', 'jumps'),\n ('fox', 'jumps', 'over'),\n ('jumps', 'over', 'the'),\n ('over', 'the', 'lazy'),\n ('the', 'lazy', 'dog')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 51
    }
   ],
   "source": [
    "list(trigrams(exmpl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "Shortcoming of Bag of Words method\n",
    "\n",
    "  - It ignores the order of the word, for example, this is bad = bad is this.\n",
    "  - It ignores the context of words. Suppose If I write the sentence \"He loved books. Education is best found in books\". It would create two vectors one for \"He loved books\" and other for \"Education is best found in books.\" It would treat both of them orthogonal which makes them independent, but in reality, they are related to each other \n",
    "\n",
    "To overcome these limitation word embedding was developed and word2vec is an approach to implement such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import abc\n",
    "\n",
    "model= Word2Vec(abc.sents())\n",
    "X= list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[('law', 0.9427725672721863),\n ('agriculture', 0.9317724108695984),\n ('policy', 0.9272750020027161),\n ('general', 0.9246857166290283),\n ('media', 0.9234775304794312),\n ('practice', 0.918496310710907),\n ('discussion', 0.9155107736587524),\n ('Crean', 0.9127512574195862),\n ('reservoir', 0.912745475769043),\n ('board', 0.9117636680603027)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 54
    }
   ],
   "source": [
    "model.wv.most_similar('science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'you'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 55
    }
   ],
   "source": [
    "model.wv.doesnt_match('See you later, thanks for visiting'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 4.14430410e-01,  1.80747733e-01, -2.75056157e-03,  6.97642267e-01,\n        2.05395311e-01,  1.51594058e-02, -1.66266590e-01, -7.66703859e-02,\n        6.34569049e-01,  5.61930120e-01, -3.11231256e-01, -5.48810422e-01,\n       -4.56843019e-01, -9.38896462e-03,  5.06384313e-01,  5.34436805e-03,\n        1.49948085e-02, -1.67875379e-01,  4.75083590e-01,  5.38990855e-01,\n       -2.08908051e-01,  6.44232407e-02, -1.21185161e-01,  3.31517644e-02,\n        1.69473827e-01, -5.44748595e-03, -1.78148359e-01, -1.21822000e-01,\n       -3.76267493e-01, -2.11002752e-02, -1.83730677e-01, -5.85076809e-02,\n        3.79859120e-01,  2.92883128e-01, -2.29618147e-01, -5.42393208e-01,\n        3.95428866e-01, -2.19907045e-01,  3.58220726e-01, -2.23434880e-01,\n       -2.11927146e-01, -3.57351899e-01,  3.39925051e-01,  5.13522208e-01,\n        1.01923060e+00,  5.63910492e-02, -4.09606755e-01,  8.93156696e-03,\n        2.13833153e-01,  3.22551727e-01, -1.55112073e-01,  1.36063173e-01,\n        4.83329557e-02,  5.97232461e-01, -3.59032720e-01,  1.36599112e-02,\n        1.28266051e-01,  5.93455255e-01,  3.81789804e-01,  5.93470454e-01,\n        6.15493977e-04, -2.13709012e-01,  2.19035864e-01,  1.08770579e-01,\n        1.65609911e-01,  3.75543386e-01, -5.70508003e-01, -1.67486235e-01,\n        4.00416702e-01, -9.54433605e-02, -7.23171085e-02, -3.92661422e-01,\n        2.00322658e-01,  4.02091980e-01, -8.64735693e-02, -3.69036086e-02,\n        2.03242287e-01,  1.22825652e-01,  3.44498575e-01,  4.82637510e-02,\n       -1.83525234e-01,  4.35495883e-01, -2.07512200e-01,  4.31904107e-01,\n        9.04957056e-02,  4.75707114e-01,  2.24846274e-01,  3.65134686e-01,\n       -6.08435333e-01,  4.64427918e-02, -5.08050025e-01,  5.34887612e-01,\n       -1.30842075e-01,  4.21132743e-01, -2.49208316e-01,  2.11147696e-01,\n       -2.75915027e-01,  2.66865790e-01,  9.75818709e-02, -3.90538424e-01],\n      dtype=float32)"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 56
    }
   ],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.93239856"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 57
    }
   ],
   "source": [
    "model.wv.similarity('man','woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "Open class words | Closed class words | Other\n",
    "--- | --- | ---\n",
    "ADJ | ADP | PUNCT\n",
    "ADV | AUX | SYM\n",
    "INTJ | CCONJ | X\n",
    "NOUN | DET |\t \n",
    "PROPN | NUM |\n",
    "VERB |PART |\t \n",
    "PRON |  \n",
    "SCONJ |\n",
    "\n",
    "#### VERB: verb\n",
    "A verb is a member of the syntactic class of words that typically signal events and actions, can constitute a minimal predicate in a clause, and govern the number and types of other constituents which may occur in the clause. Verbs are often associated with grammatical categories like tense, mood, aspect and voice, which can either be expressed inflectionally or using auxilliary verbs or particles.\n",
    "\n",
    "<p style=\"text-align: center;\">What is the verb in this sentence?</p>\n",
    "\n",
    "\n",
    "#### AUX: auxiliary\n",
    "An auxiliary is a function word that accompanies the lexical verb of a verb phrase and expresses grammatical distinctions not carried by the lexical verb, such as person, number, tense, mood, aspect, voice or evidentiality. It is often a verb (which may have non-auxiliary uses as well) but many languages have nonverbal TAME markers and these should also be tagged AUX. The class AUX also include copulas (in the narrow sense of pure linking words for nonverbal predication).\n",
    "\n",
    "Examples\n",
    "   - Tense auxiliaries: *has* (done), is (doing), will (do)\n",
    "   - Passive auxiliaries: *was* (done), *got* (done)\n",
    "   - Modal auxiliaries: *should* (do), *must* (do)\n",
    "   - Verbal copulas: He *is* a teacher.\n",
    "\n",
    "#### NOUN: noun\n",
    "Nouns are a part of speech typically denoting a person, place, thing, animal or idea.\n",
    "\n",
    "The NOUN tag is intended for common nouns only. See PROPN for proper nouns and PRON for pronouns.\n",
    "\n",
    "Examples\n",
    "  - girl\n",
    "  - cat\n",
    "  - tree\n",
    "  \n",
    "#### PROPN: proper noun  \n",
    "A proper noun is a noun (or nominal content word) that is the name (or part of the name) of a specific individual, place, or object.\n",
    "\n",
    "Acronyms of proper nouns, such as UN and NATO, should be tagged PROPN.\n",
    "\n",
    "#### ADJ: adjective\n",
    "Adjectives are words that typically modify nouns and specify their properties or attributes:\n",
    " - The car is *green*.\n",
    "\n",
    "Numbers vs. Adjectives: In general, cardinal numbers receive the part of speech NUM, while ordinal numbers (more precisely adjectival ordinal numerals) receive the tag ADJ.\n",
    "\n",
    "Examples:\n",
    "  - big\n",
    "  - old\n",
    "  - green\n",
    "  - African\n",
    "  - incomprehensible\n",
    "  - first, second, third\n",
    "\n",
    "#### ADP: adposition\n",
    "Adposition is a cover term for prepositions and postpositions. Adpositions belong to a closed set of items that occur before (preposition) or after (postposition) a complement composed of a noun phrase, noun, pronoun, or clause that functions as a noun phrase, and that form a single structure with the complement to express its grammatical and semantic relation to another unit within a clause.\n",
    "\n",
    "Examples\n",
    "  - in\n",
    "  - to\n",
    "  - during\n",
    "\n",
    "#### ADV: adverb\n",
    "Adverbs are words that typically modify verbs for such categories as time, place, direction or manner. They may also modify adjectives and other adverbs, as in *very briefly* or *arguably wrong*.\n",
    "\n",
    "Examples\n",
    "   - very\n",
    "   - well\n",
    "   - exactly\n",
    "   - tomorrow\n",
    "   - up, down\n",
    "   - interrogative adverbs: where, when, how, why\n",
    "   - demonstrative adverbs: here, there, now, then\n",
    "   - indefinite adverbs: somewhere, sometime, anywhere, anytime\n",
    "   - totality adverbs: everywhere, always\n",
    "   - negative adverbs: nowhere, never\n",
    "\n",
    "#### NUM: numeral\n",
    "A numeral is a word, functioning most typically as a determiner, adjective or pronoun, that expresses a number and a relation to the number, such as quantity, sequence, frequency or fraction.\n",
    "\n",
    "#### DET: determiner\n",
    "Determiners are words that modify nouns or noun phrases and express the reference of the noun phrase in context. That is, a determiner may indicate whether the noun is referring to a definite or indefinite element of a class, to a closer or more distant element, to an element belonging to a specified person or thing, to a particular number or quantity, etc.\n",
    "\n",
    "Examples\n",
    "   - articles (a closed class indicating definiteness, specificity or givenness): a, an, the\n",
    "   - possessive determiners (which modify a nominal): my, your\n",
    "   - demonstrative determiners: *this* as in I saw *this* car yesterday.\n",
    "   - interrogative determiners: *which* as in \"*Which* car do you like?\"\n",
    "   - relative determiners: *which* as in \"I wonder *which* car you like.\"\n",
    "   - quantity determiners (quantifiers): indefinite *any*, universal: *all*, and negative *no* as in \"We have *no* cars available.”\n",
    "\n",
    "#### PRON: pronoun\n",
    "Pronouns are words that substitute for nouns or noun phrases, whose meaning is recoverable from the linguistic or extralinguistic context.\n",
    "See also general principles on pronominal words for more tips on how to define pronouns. In particular:\n",
    "  - Non-possessive personal, reflexive or reciprocal pronouns are always tagged PRON.\n",
    "  - Possessives vary across languages. In some languages the above tests put them in the DET category. In others, they are more like a normal personal pronoun in a specific case (often the genitive), or a personal pronoun with an adposition; they are tagged PRON.\n",
    "\n",
    "#### INTJ: interjection\n",
    "An interjection is a word that is used most often as an exclamation or part of an exclamation. It typically expresses an emotional reaction, is not syntactically related to other accompanying expressions, and may include a combination of sounds not otherwise found in the language.\n",
    "\n",
    "Examples\n",
    "   - psst\n",
    "   - ouch\n",
    "   - bravo\n",
    "   - hello\n",
    "   \n",
    "#### CCONJ: coordinating conjunction\n",
    "A coordinating conjunction is a word that links words or larger constituents without syntactically subordinating one to the other and expresses a semantic relationship between them.\n",
    "\n",
    "Examples\n",
    "   - and\n",
    "   - or\n",
    "   - but\n",
    "\n",
    "#### SCONJ: subordinating conjunction\n",
    "A subordinating conjunction is a conjunction that links constructions by making one of them a constituent of the other. The subordinating conjunction typically marks the incorporated constituent which has the status of a (subordinate) clause.\n",
    "\n",
    "Examples\n",
    "    - *that* as in I believe *that* he will come.\n",
    "    - if\n",
    "    - while\n",
    "\n",
    "#### SYM: symbol\n",
    "A symbol is a word-like entity that differs from ordinary words by form, function, or both.\n",
    "\n",
    "Many symbols are or contain special non-alphanumeric characters, similarly to punctuation. What makes them different from punctuation is that they can be substituted by normal words. This involves all currency symbols, e.g. $ 75 is identical to seventy-five dollars.\n",
    "\n",
    "#### PUNCT: punctuation\n",
    "Punctuation marks are non-alphabetical characters and character groups used in many languages to delimit linguistic units in printed text.\n",
    "\n",
    "Punctuation is not taken to include logograms such as $, \\%, and §, which are instead tagged as SYM.\n",
    "\n",
    "#### X: other\n",
    "The tag X is used for words that for some reason cannot be assigned a real part-of-speech category. It should be used very restrictively.\n",
    "\n",
    "\n",
    "[Source](https://universaldependencies.org/u/pos/), note it has the above definition on many different languages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package universal_tagset to C:\\Users\\Flurin\n[nltk_data]     Hidber\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\Flurin Hidber\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\nthe cat with the hat sat in the mat\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "[('the', 'DET'),\n ('cat', 'NOUN'),\n ('with', 'ADP'),\n ('the', 'DET'),\n ('hat', 'NOUN'),\n ('sat', 'VERB'),\n ('in', 'ADP'),\n ('the', 'DET'),\n ('mat', 'NOUN')]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 58
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "\n",
    "print(txt.text.values[0])\n",
    "pos_tag(word_tokenize(txt.text.values[0]), tagset='universal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple example of text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news = fetch_20newsgroups(subset='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(news.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[rec.sport.hockey]:\t\t \"From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu> ...\"\n[comp.sys.ibm.pc.hardware]:\t\t \"From: mblawson@midway.ecn.uoknor.edu (Matthew B Lawson) ...\"\n[talk.politics.mideast]:\t\t \"From: hilmi-er@dsv.su.se (Hilmi Eren) ...\"\n[comp.sys.ibm.pc.hardware]:\t\t \"From: guyd@austin.ibm.com (Guy Dawson) ...\"\n[comp.sys.mac.hardware]:\t\t \"From: Alexander Samuel McDiarmid <am2o+@andrew.cmu.edu> ...\"\n[sci.electronics]:\t\t \"From: tell@cs.unc.edu (Stephen Tell) ...\"\n[comp.sys.mac.hardware]:\t\t \"From: lpa8921@tamuts.tamu.edu (Louis Paul Adams) ...\"\n[rec.sport.hockey]:\t\t \"From: dchhabra@stpl.ists.ca (Deepak Chhabra) ...\"\n[rec.sport.hockey]:\t\t \"From: dchhabra@stpl.ists.ca (Deepak Chhabra) ...\"\n[talk.religion.misc]:\t\t \"From: arromdee@jyusenkyou.cs.jhu.edu (Ken Arromdee) ...\"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for text, num_label in zip(news.data[:10], news.target[:10]):\n",
    "    print('[%s]:\\t\\t \"%s ...\"' % (news.target_names[num_label], text[:100].split('\\n')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train(classifier, X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=33)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    print(\"Accuracy: %s\" % classifier.score(X_test, y_test))\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy: 0.8463497453310697\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 63
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "trial1 = Pipeline(\n",
    "    [('vectorizer', TfidfVectorizer()),\n",
    "     ('classifier', MultinomialNB()),])\n",
    "train(trial1, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy: 0.8777589134125636\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 64
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "trial2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "train(trial2, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#Drop all words that appear in less than 5% of the documents alpha = 0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Accuracy: 0.9028013582342954\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...     vocabulary=None)), ('classifier', MultinomialNB(alpha=0.05, class_prior=None, fit_prior=True))])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 66
    }
   ],
   "source": [
    "trial3 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(stop_words=stopwords.words('english'),\n",
    "min_df=5)),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial3, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Flurin Hidber\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", '``', 'abov', 'ani', 'becaus', 'befor', 'could', 'doe', 'dure', 'ha', 'hi', 'might', 'must', \"n't\", 'need', 'onc', 'onli', 'ourselv', 'sha', 'themselv', 'thi', 'veri', 'wa', 'whi', 'wo', 'would', 'yourselv'] not in stop_words.\n  sorted(inconsistent))\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Accuracy: 0.9108658743633277\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "Pipeline(memory=None,\n     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=...rue, vocabulary=None)), ('classifier', MultinomialNB(alpha=0.05, class_prior=None, fit_prior=True))])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 67
    }
   ],
   "source": [
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def stemming_tokenizer(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text)]\n",
    "\n",
    "trial4 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(tokenizer=stemming_tokenizer, stop_words=stopwords.words('english') + list(string.punctuation))),\n",
    "    ('classifier', MultinomialNB(alpha=0.05)),\n",
    "])\n",
    "train(trial4, news.data, news.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional resources\n",
    "\n",
    "### LDA\n",
    "A good visualization tool for LDA topics and keywords is LDAVis in R that has been ported to python [pyLDAvis](https://github.com/bmabey/pyLDAvis).\n",
    "\n",
    "[A good notebook on LDA](http://nbviewer.jupyter.org/github/bmabey/hacker_news_topic_modelling/blob/master/HN%20Topic%20Model%20Talk.ipynb).\n",
    "\n",
    "### PubMed artcile classification\n",
    "A [model](https://github.com/melcutz/NLU_tutorial/blob/master/3_spacy_pubmed_model.ipynb) that is able to tell if a Pubmed article is refering to child or adult patient(s).\n",
    "\n",
    "### POS tagging\n",
    "[Negation detection](https://github.com/melcutz/NLP-demo-2017/blob/master/SpaCy_Intro.ipynb) using POS tagging and syntactic dependencies.\n",
    "\n",
    "### Biopython\n",
    "[Biopython](http://biopython.org/DIST/docs/tutorial/Tutorial.html) has a vast amount of tutorial on biomedical data processing using python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}